{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Nivelamento</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 1 - 1.5 Pontos\n",
    "### Avalie as sentenças abaixo como verdadeiras ou falsas:\n",
    "- a. HiveQL é uma linguagem de consulta semelhante ao SQL.\n",
    "- b. Apache NiFi é uma ferramenta de ingestão de dados muito versátil.\n",
    "- c. As linguagens de programação Python e C# são suportadas pelo Apache Spark.\n",
    "- d. Databricks é uma plataforma que pode ser combinada apenas com Microsoft Azure.\n",
    "- e. MapReduce é um framework para processamento distribuído."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Respostas:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a - Verdadeiro - HiveQL é uma linguagem que se assemelha ao SQL Base ou SQL Ansi e é utilizada pelo ecossistema hive.\n",
    "\n",
    "b - Verdadeiro - NiFi é uma ferramenta de mercado muito interessante que permite a ingestão de dados de forma rápida e confiável, porém vale lembrar que o NiFi por si tem muitos mais atribuições podendo agir como um direcionador e automatizador de fluxos de dados completos, incluindo os processos de transformação.\n",
    "\n",
    "c - Verdadeiro - O apache spark pode ser utilizado com um número significativo de linguagens, dentre elas: Python, C#, Java, Scala (Linguagem em que o framework foi escrito), R , lembrando que como o framework é baseado em Scala que por sua vez executa sob a JVM, as Linguagens Java e Scala executam o Spark nativamente as demais Necessitam de uma Camada de integração, o ponto legal, é que isso não necessariamente significa uma degradação de performance pelo menos para as features nativas do framework\n",
    "\n",
    "d - Falso - Databricks é uma plataform que fornece um ambiente gerenciado para provisionamento de clusters spark que pode ser utilizada com GCP, AWS e Azure Cloud\n",
    "\n",
    "e - Verdadeiro - O MapReduce foi o precursor das ferramentas de big data, executando sob o ambiente a hadoop foi utilizado por muito tempo como principal framework de processamento distribuído, mais tarde, passou a ser substituído por seu sucessor Apache Spark, mas ainda é utilizado em organizações tradicionais e para a resolução de problemas específicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 2 - 1.0 Ponto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para uma empresa que almeja aumentar seu poder computacional (com escalabilidade e agilidade), mas não abre mão de reter seus dados sensíveis em uma arquitetura on-premise, qual modelo de computação em nuvem você proporia?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resposta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para esse cenário uma nuvem híbrida se apresenta como uma solução plausível, onde os componentes on-premises ficaram responsáveis por armazenar e processar os dados sensíveis, podemos disponbilizá-los de maneira anonimizada e filtrada para que as estruturas hospedadas na nuvem pudessem os consultar e para que houvesse interoperabilidade e extração dos valores dos dados sem o comprometimento da segurança"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 3 - 1.5 Pontos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vencedor\n",
    "Baseado nas tabelas, assinale a alternativa com o resultado da query descrita.\n",
    "\n",
    "**Consulta não liberada.**\n",
    "\n",
    "<center><img src=\"https://s3-sa-east-1.amazonaws.com/lcpi/7dbafb1c-863b-405f-b602-ca67d927cd17.png\" width=\"50%\"/></center>\n",
    "\n",
    "```sql\n",
    "SELECT id FROM runners\n",
    "WHERE id NOT IN (SELECT winner_id FROM races)\n",
    "```\n",
    "\n",
    "- a) 1 4 5\n",
    "- b) 2 3 2\n",
    "- c) Null\n",
    "- d) John Doe; Alice Jones; Bobby Louis.\n",
    "- e) 1 2 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resposta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resposta Correta - Letra A (1 4 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 4 - 3.0 Pontos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tendo a tabela abaixo como base:\n",
    "<center><img src=\"https://s3-sa-east-1.amazonaws.com/lcpi/729a24e6-fe6f-4474-8063-32d7658b187d.JPG\" width=\"50%\"/></center>\n",
    "\n",
    "Qual o conteúdo da linha faltando no código:\n",
    "\n",
    "```sql\n",
    "SELECT \n",
    "\tcategoria,\n",
    "\tnome_produto,\n",
    "\tpreco,\n",
    "\t--linha faltando\n",
    "FROM produtos\n",
    "ORDER BY categoria;\n",
    "```\n",
    "\n",
    "Para que o seguinte relatório seja produzido?\n",
    "\n",
    "<center><img src=\"https://s3-sa-east-1.amazonaws.com/lcpi/75454d89-0fbb-4442-9cb6-838823bfb058.JPG\" width=\"50%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resposta:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rank() over(partition by categoria order by preco desc) as posicao_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 5 - 3.0 Pontos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O dataset a seguir é fruto de dados coletados por um dispositivo de frequência cardíaca. Além do id do dispositivo, a frequência cardíaca e o nome do paciente, há uma coluna \"time\" que possui valores do tipo unixtime.\n",
    "\n",
    "<center><img src=\"https://s3-sa-east-1.amazonaws.com/lcpi/8a2fb10a-f114-4c01-8306-57eac725fefa.JPG\" width=\"50%\"/></center>\n",
    "\n",
    "Para análises futuras, a transformação necessária será criar duas novas colunas: uma contendo a data, \"dte\", e outra com o timestamp chamada \"time\". Ao final, salvar este dataset em parquet. Tudo utilizando PySpark pelo alto poder de processamento utilizando um cluster.\n",
    "\n",
    "Complete as linhas faltantes no código abaixo para atingir o resultado mencionado acima.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import col, from_unixtime\n",
    "\n",
    "def process_health_tracker_data(dataframe):\n",
    "  return (\n",
    "    dataframe\n",
    "    .select(\n",
    "        from_unixtime(\"time\").cast(\"date\").alias(\"dte\"),\n",
    "        #linha faltando,\n",
    "        \"heartrate\",\n",
    "        \"name\",\n",
    "        col(\"device_id\").cast(\"integer\").alias(\"p_device_id\")\n",
    "    )\n",
    "  )\n",
    "\n",
    "processedDF = process_health_tracker_data(health_tracker_data_2020_1_df)\n",
    "```\n",
    "```python\n",
    "(processedDF.write\n",
    " .mode(\"overwrite\")\n",
    " #linha faltando\n",
    " .partitionBy(\"p_device_id\")\n",
    " .save(\"/trusted\"))\n",
    " ```\n",
    "\n",
    " <center><img src=\"https://s3-sa-east-1.amazonaws.com/lcpi/cc935939-3be1-4b56-a970-315e45948a44.JPG\" width=\"50%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respostas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from pyspark.sql.functions import col, from_unixtime\n",
    "\n",
    "def process_health_tracker_data(dataframe):\n",
    "  return (\n",
    "    dataframe\n",
    "    .select(\n",
    "        from_unixtime(\"time\").cast(\"date\").alias(\"dte\"),\n",
    "        from_unixtime(\"time\").cast(\"timestamp\").alias(\"time\"),\n",
    "        col(\"time\").alias(\"unixtime_column\"), #Adicionado apenas para fins de necessidades futuras ...\n",
    "        \"heartrate\",\n",
    "        \"name\",\n",
    "        col(\"device_id\").cast(\"integer\").alias(\"p_device_id\")\n",
    "    )\n",
    "  )\n",
    "\n",
    "processedDF = process_health_tracker_data(health_tracker_data_2020_1_df)\n",
    "```\n",
    "```python\n",
    "(processedDF.write\n",
    " .mode(\"overwrite\")\n",
    " .format(\"parquet\")\n",
    " .partitionBy(\"p_device_id\")\n",
    " .save(\"/trusted\"))\n",
    " ```"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "08ec9c73fadad94ce99ddb0e76c3e0957b6dd2cf9bc0d906e5101483ecd7e830"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
